{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f268ff43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_community in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (0.2.14)\n",
      "Requirement already satisfied: tiktoken in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (0.7.0)\n",
      "Requirement already satisfied: langchain-openai in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (0.1.22)\n",
      "Requirement already satisfied: langchainhub in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (0.1.21)\n",
      "Requirement already satisfied: chromadb in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (0.5.3)\n",
      "Requirement already satisfied: langchain in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (0.2.15)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from langchain_community) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from langchain_community) (2.0.28)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from langchain_community) (3.8.5)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: langchain-core==0.2.36 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from langchain_community) (0.2.36)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from langchain_community) (0.1.104)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from langchain_community) (1.24.3)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from langchain_community) (2.31.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from langchain_community) (8.5.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from langchain-core==0.2.36->langchain_community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from langchain-core==0.2.36->langchain_community) (24.1)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from langchain-core==0.2.36->langchain_community) (2.7.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from langchain-core==0.2.36->langchain_community) (4.12.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from tiktoken) (2022.7.9)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.40.0 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from langchain-openai) (1.42.0)\n",
      "Requirement already satisfied: types-requests<3.0.0.0,>=2.31.0.2 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from langchainhub) (2.32.0.20240712)\n",
      "Requirement already satisfied: build>=1.0.3 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from chromadb) (1.2.1)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.3 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from chromadb) (0.7.3)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from chromadb) (0.110.3)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.27.1)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from chromadb) (3.5.2)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from chromadb) (1.19.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from chromadb) (1.26.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from chromadb) (1.26.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from chromadb) (0.47b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from chromadb) (1.26.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from chromadb) (0.13.2)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from chromadb) (4.65.0)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from chromadb) (6.1.1)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from chromadb) (1.62.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from chromadb) (4.2.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from chromadb) (0.12.3)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from chromadb) (30.1.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from chromadb) (4.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from chromadb) (3.10.3)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from chromadb) (0.27.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from langchain) (0.2.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.2.0)\n",
      "Requirement already satisfied: pyproject_hooks in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from build>=1.0.3->chromadb) (1.1.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.22.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from fastapi>=0.95.2->chromadb) (0.37.2)\n",
      "Requirement already satisfied: anyio in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from httpx>=0.27.0->chromadb) (3.5.0)\n",
      "Requirement already satisfied: certifi in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from httpx>=0.27.0->chromadb) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from httpx>=0.27.0->chromadb) (1.0.4)\n",
      "Requirement already satisfied: idna in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from httpx>=0.27.0->chromadb) (3.6)\n",
      "Requirement already satisfied: sniffio in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from httpx>=0.27.0->chromadb) (1.2.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2.34.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (0.58.0)\n",
      "Requirement already satisfied: requests-oauthlib in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: coloredlogs in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (24.3.7)\n",
      "Requirement already satisfied: protobuf in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (4.25.3)\n",
      "Requirement already satisfied: sympy in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (1.11.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (0.5.0)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\n",
      "Requirement already satisfied: importlib-metadata<=8.0.0,>=6.0 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from opentelemetry-api>=1.2.0->chromadb) (7.0.1)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.63.2)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.26.0 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.26.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.26.0 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.26.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.47b0 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.47b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.47b0 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.47b0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.47b0 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.47b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.47b0 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.47b0)\n",
      "Requirement already satisfied: setuptools>=16.0 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from opentelemetry-instrumentation==0.47b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (69.5.1)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from opentelemetry-instrumentation==0.47b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.14.1)\n",
      "Requirement already satisfied: asgiref~=3.0 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from opentelemetry-instrumentation-asgi==0.47b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from posthog>=2.4.0->chromadb) (1.6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: backoff>=1.10.0 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from pydantic<3,>=1->langchain-core==0.2.36->langchain_community) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.3 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from pydantic<3,>=1->langchain-core==0.2.36->langchain_community) (2.18.3)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from typer>=0.9.0->chromadb) (13.7.0)\n",
      "Requirement already satisfied: httptools>=0.5.0 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.22.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (11.0.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.7.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from importlib-metadata<=8.0.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.11.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core==0.2.36->langchain_community) (2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (2.15.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb) (0.1.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/madhushantan/anaconda3/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.8)\n"
     ]
    }
   ],
   "source": [
    "! pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5581e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "········\n",
      "········\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = getpass.getpass()\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a4ba092",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "blog_docs = loader.load()\n",
    "\n",
    "# Split\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300, \n",
    "    chunk_overlap=50)\n",
    "\n",
    "# Make splits\n",
    "splits = text_splitter.split_documents(blog_docs)\n",
    "\n",
    "# Index\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                    embedding=OpenAIEmbeddings())\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d0409b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Multi Query: Different Perspectives\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_perspectives \n",
    "    | ChatOpenAI(temperature=0) \n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73565ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kq/90ww7__n28d6svqwm1clq9g00000gn/T/ipykernel_81641/2784938305.py:10: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  return [loads(doc) for doc in unique_docs]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    # Flatten list of lists, and convert each Document to string\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    # Get unique documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Return\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "# Retrieve\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "docs = retrieval_chain.invoke({\"question\":question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "35870c62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition for LLM agents involves breaking down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks. This process allows the agent to utilize more test-time computation to decompose hard tasks into smaller and simpler steps, ultimately improving the quality of the final results.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c984bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# RAG-Fusion: Related\n",
    "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (4 queries):\"\"\"\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ec7fb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_rag_fusion \n",
    "    | ChatOpenAI(temperature=0)\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25f7b41a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents \n",
    "        and an optional parameter k used in the RRF formula \"\"\"\n",
    "    \n",
    "    # Initialize a dictionary to hold fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
    "            doc_str = dumps(doc)\n",
    "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # Retrieve the current score of the document, if any\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
    "    return reranked_results\n",
    "\n",
    "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac658ba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition for LLM agents involves breaking down complex tasks into smaller and simpler steps to enhance model performance. This can be achieved through techniques like Chain of Thought (CoT) and Tree of Thoughts, which prompt the model to think step by step and explore multiple reasoning possibilities at each step. Task decomposition can be done through simple prompting, task-specific instructions, or with human inputs.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain_rag_fusion, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25859cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Decomposition\n",
    "template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
    "The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (3 queries):\"\"\"\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cf196fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Chain\n",
    "generate_queries_decomposition = ( prompt_decomposition | llm | StrOutputParser() | (lambda x: x.split(\"\\n\")))\n",
    "\n",
    "# Run\n",
    "question = \"What are the main components of an LLM-powered autonomous agent system?\"\n",
    "questions = generate_queries_decomposition.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53338d4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. What is LLM technology and how does it work in autonomous agent systems?',\n",
       " '2. What are the specific components that make up an LLM-powered autonomous agent system?',\n",
       " '3. How do the main components of an LLM-powered autonomous agent system interact with each other to enable autonomous functionality?']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f286a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "template = \"\"\"Here is the question you need to answer:\n",
    "\n",
    "\\n --- \\n {question} \\n --- \\n\n",
    "\n",
    "Here is any available background question + answer pairs:\n",
    "\n",
    "\\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "\n",
    "Here is additional context relevant to the question: \n",
    "\n",
    "\\n --- \\n {context} \\n --- \\n\n",
    "\n",
    "Use the above context and any background question + answer pairs to answer the question: \\n {question}\n",
    "\"\"\"\n",
    "\n",
    "decomposition_prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c55d82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def format_qa_pair(question, answer):\n",
    "    \"\"\"Format Q and A pair\"\"\"\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    formatted_string += f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "# llm\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "q_a_pairs = \"\"\n",
    "for q in questions:\n",
    "    \n",
    "    rag_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | retriever, \n",
    "     \"question\": itemgetter(\"question\"),\n",
    "     \"q_a_pairs\": itemgetter(\"q_a_pairs\")} \n",
    "    | decomposition_prompt\n",
    "    | llm\n",
    "    | StrOutputParser())\n",
    "\n",
    "    answer = rag_chain.invoke({\"question\":q,\"q_a_pairs\":q_a_pairs})\n",
    "    q_a_pair = format_qa_pair(q,answer)\n",
    "    q_a_pairs = q_a_pairs + \"\\n---\\n\"+  q_a_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7fc6369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The main components of an LLM-powered autonomous agent system interact with each other in a cohesive manner to enable autonomous functionality. \\n\\n1. Planning: The planning component involves task decomposition, where the agent breaks down large tasks into smaller, manageable subgoals. Techniques like Chain of Thought and Tree of Thoughts are used to enhance model performance on complex tasks by decomposing them into simpler steps. This planning process helps the agent understand the steps needed to achieve a goal.\\n\\n2. Reflection and Refinement: This component allows the agent to engage in self-criticism and self-reflection over past actions, learn from mistakes, and refine its strategies for future tasks. By reflecting on its actions and refining its approaches, the agent can continuously improve its performance and decision-making processes.\\n\\n3. Interaction with the Environment: Through the integration of reasoning and acting within LLM, the agent can interact with the environment using task-specific discrete actions and language prompts. This interaction enables the agent to perform actions like using the Wikipedia search API and generate reasoning traces in natural language. By interacting effectively with the environment, the agent can execute tasks and adapt to changing circumstances.\\n\\nOverall, these components work together harmoniously to enable the LLM-powered autonomous agent system to solve complex problems, improve iteratively, and operate autonomously in various scenarios.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1c58c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step - back\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Could the members of The Police perform lawful arrests?\",\n",
    "        \"output\": \"what can the members of The Police do?\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Jan Sindel’s was born in what country?\",\n",
    "        \"output\": \"what is Jan Sindel’s personal history?\",\n",
    "    },\n",
    "]\n",
    "# We now transform these to example messages\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(example_prompt =example_prompt, examples = examples)\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:\"\"\",\n",
    "        ),\n",
    "        # Few shot examples\n",
    "        few_shot_prompt,\n",
    "        # New question\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0052041b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the process of breaking down tasks for LLM agents?'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_queries_step_back = prompt | ChatOpenAI(temperature=0) | StrOutputParser()\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "generate_queries_step_back.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6ddd7c57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition for LLM agents refers to the process of breaking down complex tasks into smaller, more manageable subgoals or steps. This approach allows the agent to efficiently handle intricate tasks by dividing them into simpler components. Task decomposition is essential for LLM-powered autonomous agents as it enables them to plan ahead and navigate through various stages of a task systematically.\\n\\nIn the context of LLM agents, task decomposition can be achieved through various techniques such as Chain of Thought (CoT) and Tree of Thoughts. CoT involves prompting the model to \"think step by step\" and decompose hard tasks into smaller steps, making them more manageable. On the other hand, Tree of Thoughts extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure of thought processes.\\n\\nAdditionally, task decomposition can be facilitated by providing LLM agents with simple prompts like \"Steps for XYZ\" or task-specific instructions tailored to the nature of the task at hand. Human inputs can also be utilized for task decomposition, especially in scenarios where domain expertise or creative thinking is required.\\n\\nOverall, task decomposition plays a crucial role in enhancing the performance of LLM agents by breaking down complex tasks into smaller, more manageable components, allowing for efficient planning and execution.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnableLambda\n",
    "response_prompt_template = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.\n",
    "\n",
    "# {normal_context}\n",
    "# {step_back_context}\n",
    "\n",
    "# Original Question: {question}\n",
    "# Answer:\"\"\"\n",
    "response_prompt = ChatPromptTemplate.from_template(response_prompt_template)\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        # Retrieve context using the normal question\n",
    "        \"normal_context\": RunnableLambda(lambda x: x[\"question\"]) | retriever,\n",
    "        # Retrieve context using the step-back question\n",
    "        \"step_back_context\": generate_queries_step_back | retriever,\n",
    "        # Pass on the question\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "    }\n",
    "    | response_prompt\n",
    "    | ChatOpenAI(temperature=0)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77872d23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
